{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How RL differs from other ML paradigms?\n",
    "\n",
    "We can categorize ML into three types:\n",
    "* Supervised learning\n",
    "* Unsupervised learning\n",
    "* Reinforcement learning\n",
    "\n",
    "In supervised learning, the machine learns from training data. The training data\n",
    "consists of a labeled pair of inputs and outputs. So, we train the model (agent)\n",
    "using the training data in such a way that the model can generalize its learning to\n",
    "new unseen data. It is called supervised learning because the training data acts as a\n",
    "supervisor, since it has a labeled pair of inputs and outputs, and it guides the model\n",
    "in learning the given task.\n",
    "\n",
    "Now, let's understand the difference between supervised and reinforcement learning\n",
    "with an example. Consider the dog analogy we discussed earlier in the chapter. In\n",
    "supervised learning, to teach the dog to catch a ball, we will teach it explicitly by\n",
    "specifying turn left, go right, move forward seven steps, catch the ball, and so on\n",
    "in the form of training data. But in RL, we just throw a ball, and every time the dog\n",
    "catches the ball, we give it a cookie (reward). So, the dog will learn to catch the ball\n",
    "while trying to maximize the cookies (reward) it can get.\n",
    "\n",
    "Let's consider one more example. Say we want to train the model to play chess using\n",
    "supervised learning. In this case, we will have training data that includes all the\n",
    "moves a player can make in each state, along with labels indicating whether it is a\n",
    "good move or not. Then, we train the model to learn from this training data, whereas\n",
    "in the case of RL, our agent will not be given any sort of training data; instead, we\n",
    "just give a reward to the agent for each action it performs. Then, the agent will learn\n",
    "by interacting with the environment and, based on the reward it gets, it will choose\n",
    "its actions.\n",
    "\n",
    "Similar to supervised learning, in unsupervised learning, we train the model (agent)\n",
    "based on the training data. But in the case of unsupervised learning, the training data\n",
    "does not contain any labels; that is, it consists of only inputs and not outputs. The\n",
    "goal of unsupervised learning is to determine hidden patterns in the input. There is\n",
    "a common misconception that RL is a kind of unsupervised learning, but it is not. In\n",
    "unsupervised learning, the model learns the hidden structure, whereas, in RL, the\n",
    "model learns by maximizing the reward.\n",
    "\n",
    "For instance, consider a movie recommendation system. Say we want to recommend\n",
    "a new movie to the user. With unsupervised learning, the model (agent) will find\n",
    "movies similar to the movies the user (or users with a profile similar to the user) has\n",
    "viewed before and recommend new movies to the user.\n",
    "\n",
    "With RL, the agent constantly receives feedback from the user. This feedback\n",
    "represents rewards (a reward could be ratings the user has given for a movie they\n",
    "have watched, time spent watching a movie, time spent watching trailers, and so on).\n",
    "Based on the rewards, an RL agent will understand the movie preference of the user\n",
    "and then suggest new movies accordingly.\n",
    "\n",
    "Since the RL agent is learning with the aid of rewards, it can understand if the user's\n",
    "movie preference changes and suggest new movies according to the user's changed\n",
    "movie preference dynamically.\n",
    "\n",
    "Thus, we can say that in both supervised and unsupervised learning the model\n",
    "(agent) learns based on the given training dataset, whereas in RL the agent learns\n",
    "by directly interacting with the environment. Thus, RL is essentially an interaction\n",
    "between the agent and its environment.\n",
    "\n",
    "Before moving on to the fundamental concepts of RL, we will introduce a popular\n",
    "process to aid decision-making in an RL environment.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
